{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN_STEPS = 500\n",
    "MODEL_TYPE = 'ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18'\n",
    "CONFIG_TYPE = 'ssd_mobilenet_v1_quantized_300x300_coco14_sync'\n",
    "\n",
    "CHECKPOINT_PATH = '/content/checkpoint'\n",
    "OUTPUT_PATH = '/content/output'\n",
    "\n",
    "LABEL_MAP_PATH = '/content/label_map.pbtxt'\n",
    "TRAIN_RECORD_PATH = '/content/annotations/train.record'\n",
    "VAL_RECORD_PATH   = '/content/annotations/test.record'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "# Clone the tensorflow models repository if it doesn't already exist\n",
    "if \"models\" in pathlib.Path.cwd().parts:\n",
    "  while \"models\" in pathlib.Path.cwd().parts:\n",
    "    os.chdir('..')\n",
    "elif not pathlib.Path('models').exists():\n",
    "  !git clone --depth 1 https://github.com/cloud-annotations/models\n",
    "\n",
    "!pip install cloud-annotations==0.0.4\n",
    "!pip install tf_slim\n",
    "!pip install lvis\n",
    "!pip install --no-deps tensorflowjs==1.4.0\n",
    "\n",
    "%cd /content/models/research\n",
    "!protoc object_detection/protos/*.proto --python_out=.\n",
    "\n",
    "pwd = os.getcwd()\n",
    "os.environ['PYTHONPATH'] += f':{pwd}:{pwd}/slim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python object_detection/builders/model_builder_tf1_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "  random_image_gpu = tf.random_normal((100, 100, 100, 3))\n",
    "  net_gpu = tf.layers.conv2d(random_image_gpu, 32, 7)\n",
    "  net_gpu = tf.reduce_sum(net_gpu)\n",
    "  \n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "try:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  print('ok')\n",
    "except tf.errors.InvalidArgumentError:\n",
    "  print(\n",
    "      '\\n\\nThis error most likely means that this notebook is not '\n",
    "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
    "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --depth 1 git@github.com:andreaswierm/bubu-finder.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content\n",
    "\n",
    "import os\n",
    "if not os.path.exists('annotations'):\n",
    "    os.makedirs('annotations')\n",
    "\n",
    "\n",
    "!python scripts/generate_tfrecord.py -x ./../gdrive/My\\ Drive/andreas-and-bruna-images/train -l ./annotations/label_map.pbtxt -o ./annotations/train.record\n",
    "!python scripts/generate_tfrecord.py -x ./../gdrive/My\\ Drive/andreas-and-bruna-images/test -l ./annotations/label_map.pbtxt -o ./annotations/test.record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "\n",
    "import six.moves.urllib as urllib\n",
    "\n",
    "download_base = 'http://download.tensorflow.org/models/object_detection/'\n",
    "model = MODEL_TYPE + '.tar.gz'\n",
    "tmp = '/content/checkpoint.tar.gz'\n",
    "\n",
    "if not (os.path.exists(CHECKPOINT_PATH)):\n",
    "  # Download the checkpoint\n",
    "  opener = urllib.request.URLopener()\n",
    "  opener.retrieve(download_base + model, tmp)\n",
    "\n",
    "  # Extract all the `model.ckpt` files.\n",
    "  with tarfile.open(tmp) as tar:\n",
    "    for member in tar.getmembers():\n",
    "      member.name = os.path.basename(member.name)\n",
    "      if 'model.ckpt' in member.name:\n",
    "        tar.extract(member, path=CHECKPOINT_PATH)\n",
    "\n",
    "  os.remove(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from google.protobuf import text_format\n",
    "\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "pipeline_skeleton = '/content/models/research/object_detection/samples/configs/' + CONFIG_TYPE + '.config'\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_skeleton)\n",
    "\n",
    "label_map = label_map_util.get_label_map_dict(LABEL_MAP_PATH)\n",
    "num_classes = 2\n",
    "meta_arch = configs[\"model\"].WhichOneof(\"model\")\n",
    "\n",
    "override_dict = {\n",
    "  'model.{}.num_classes'.format(meta_arch): num_classes,\n",
    "  'train_config.batch_size': 24,\n",
    "  'train_input_path': TRAIN_RECORD_PATH,\n",
    "  'eval_input_path': VAL_RECORD_PATH,\n",
    "  'train_config.fine_tune_checkpoint': os.path.join(CHECKPOINT_PATH, 'model.ckpt'),\n",
    "  'label_map_path': LABEL_MAP_PATH\n",
    "}\n",
    "\n",
    "configs = config_util.merge_external_params_with_configs(configs, kwargs_dict=override_dict)\n",
    "pipeline_config = config_util.create_pipeline_proto_from_configs(configs)\n",
    "config_util.save_pipeline_config(pipeline_config, \"/content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $OUTPUT_PATH\n",
    "!python -m object_detection.model_main \\\n",
    "    --pipeline_config_path=/content/pipeline.config \\\n",
    "    --model_dir=$OUTPUT_PATH \\\n",
    "    --num_train_steps=$NUM_TRAIN_STEPS \\\n",
    "    --num_eval_steps=100"
   ]
  }
 ]
}